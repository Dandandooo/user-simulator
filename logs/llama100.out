Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Traceback (most recent call last):
  File "/taiga/illinois/collab/eng/cs/conv-ai/UserSimulator/src/prompt_llm/runllama.py", line 7, in <module>
    llama3 = HugLM(model_name)
  File "/taiga/illinois/collab/eng/cs/conv-ai/UserSimulator/src/model/llms.py", line 198, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(model_name, **config)
  File "/u/dphilipov/miniforge3/envs/Tur/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 524, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/u/dphilipov/miniforge3/envs/Tur/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 989, in from_pretrained
    return config_class.from_dict(config_dict, **unused_kwargs)
  File "/u/dphilipov/miniforge3/envs/Tur/lib/python3.10/site-packages/transformers/configuration_utils.py", line 772, in from_dict
    config = cls(**config_dict)
  File "/u/dphilipov/miniforge3/envs/Tur/lib/python3.10/site-packages/transformers/models/llama/configuration_llama.py", line 161, in __init__
    self._rope_scaling_validation()
  File "/u/dphilipov/miniforge3/envs/Tur/lib/python3.10/site-packages/transformers/models/llama/configuration_llama.py", line 182, in _rope_scaling_validation
    raise ValueError(
ValueError: `rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}
